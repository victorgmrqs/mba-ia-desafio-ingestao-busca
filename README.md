# Desafio MBA Engenharia de Software com IA - Full Cycle

Sistema de IngestÃ£o e Busca SemÃ¢ntica com LangChain e PostgreSQL (pgVector).

## DescriÃ§Ã£o

Este projeto implementa um sistema RAG (Retrieval Augmented Generation) que permite:
- **IngestÃ£o**: Ler um arquivo PDF e salvar suas informaÃ§Ãµes em um banco de dados PostgreSQL com extensÃ£o pgVector
- **Busca**: Permitir que o usuÃ¡rio faÃ§a perguntas via linha de comando (CLI) e receba respostas baseadas apenas no conteÃºdo do PDF

## Tecnologias Utilizadas

- **Linguagem**: Python 3.12+
- **Framework**: LangChain
- **Banco de dados**: PostgreSQL + pgVector
- **ContainerizaÃ§Ã£o**: Docker & Docker Compose
- **CLI**: Rich (para interface melhorada)

## PrÃ©-requisitos

- Python 3.12 ou superior
- Docker e Docker Compose
- API Key da OpenAI (ou Google Gemini)

## InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca
```

2. Crie e ative um ambiente virtual:
```bash
python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
# ou usando uv:
uv pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
```

Edite o arquivo `.env` e adicione suas API keys:
```env
OPENAI_API_KEY=sua_chave_aqui
# ou
GEMINI_API_KEY=sua_chave_aqui
```

## ConfiguraÃ§Ã£o

O projeto usa `pydantic-settings` para gerenciar configuraÃ§Ãµes. As variÃ¡veis de ambiente seguem o padrÃ£o de prefixos:

- `OPENAI_API_KEY` - API Key da OpenAI
- `OPENAI_LLM_MODEL` - Modelo LLM (padrÃ£o: `gpt-5-nano`)
- `OPENAI_EMBEDDING_MODEL` - Modelo de embeddings (padrÃ£o: `text-embedding-3-small`)
- `DATABASE_URL` - URL de conexÃ£o do PostgreSQL (padrÃ£o: `postgresql://postgres:postgres@localhost:5432/rag`)
- `DATABASE_COLLECTION_NAME` - Nome da coleÃ§Ã£o (padrÃ£o: `document_collection`)
- `PDF_PATH` - Caminho do arquivo PDF (padrÃ£o: `./document.pdf`)
- `PDF_CHUNK_SIZE` - Tamanho dos chunks (padrÃ£o: `1000`)
- `PDF_CHUNK_OVERLAP` - Overlap dos chunks (padrÃ£o: `150`)
- `SEARCH_K` - NÃºmero de resultados na busca (padrÃ£o: `10`)

## ExecuÃ§Ã£o

### 1. Subir o banco de dados

```bash
docker compose up -d
```

Isso iniciarÃ¡ um container PostgreSQL com a extensÃ£o pgVector na porta 5432.

### 2. Executar ingestÃ£o do PDF

Coloque o arquivo PDF na raiz do projeto com o nome `document.pdf` (ou configure o caminho no `.env`):

```bash
python src/ingest.py
```

O script irÃ¡:
- Carregar o PDF
- Dividir em chunks de 1000 caracteres com overlap de 150
- Criar embeddings para cada chunk
- Salvar no banco de dados PostgreSQL com pgVector

### 3. Rodar o chat

```bash
python src/chat.py
```

O sistema abrirÃ¡ uma interface CLI onde vocÃª pode fazer perguntas sobre o conteÃºdo do PDF.

**Exemplo de uso:**
```
ðŸ’¬ Pergunta: Qual o faturamento da Empresa SuperTechIABrazil?
âœ… Resposta: O faturamento foi de 10 milhÃµes de reais.
```

Para perguntas fora do contexto:
```
ðŸ’¬ Pergunta: Quantos clientes temos em 2024?
âœ… Resposta: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

Para sair, digite: `sair`, `exit` ou `quit`

## Estrutura do Projeto

```
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o do PostgreSQL com pgVector
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ pyproject.toml             # ConfiguraÃ§Ã£o do projeto (uv)
â”œâ”€â”€ .env.example               # Template de variÃ¡veis de ambiente
â”œâ”€â”€ document.pdf               # PDF para ingestÃ£o
â”œâ”€â”€ README.md                  # Este arquivo
â”œâ”€â”€ CHALLENGE.md               # EspecificaÃ§Ã£o do desafio
â””â”€â”€ src/
    â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes centralizadas (pydantic-settings)
    â”œâ”€â”€ prompts.py             # Templates de prompts
    â”œâ”€â”€ ingest.py              # Script de ingestÃ£o do PDF
    â”œâ”€â”€ search.py              # FunÃ§Ã£o de busca semÃ¢ntica
    â”œâ”€â”€ chat.py                # CLI para interaÃ§Ã£o com usuÃ¡rio
    â””â”€â”€ services/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ embeddings.py       # Gerenciamento de embeddings
        â”œâ”€â”€ llm.py             # Gerenciamento de LLM
        â””â”€â”€ vector_store.py     # Acesso ao banco vetorial
```

## Funcionalidades Implementadas

âœ… IngestÃ£o de PDF com chunks de 1000 caracteres e overlap de 150  
âœ… Armazenamento de vetores no PostgreSQL com pgVector  
âœ… Busca semÃ¢ntica com k=10 resultados mais relevantes  
âœ… CLI interativo com Rich para melhor UX  
âœ… ConfiguraÃ§Ã£o centralizada com pydantic-settings  
âœ… Suporte para OpenAI e Gemini  
âœ… Tratamento de perguntas fora do contexto  
âœ… ValidaÃ§Ãµes e tratamento de erros  

## SoluÃ§Ã£o de Problemas

### Erro: "No module named 'src'"
Certifique-se de estar executando os scripts a partir da raiz do projeto.

### Erro: "The api_key client option must be set"
Verifique se a variÃ¡vel `OPENAI_API_KEY` estÃ¡ configurada no arquivo `.env` ou como variÃ¡vel de ambiente.

### Erro: "Arquivo PDF nÃ£o encontrado"
Verifique se o arquivo `document.pdf` existe na raiz do projeto ou configure o caminho correto no `.env`.

### Erro de conexÃ£o com o banco
Certifique-se de que o Docker Compose estÃ¡ rodando:
```bash
docker compose ps
```

## Desenvolvimento

O projeto foi desenvolvido seguindo boas prÃ¡ticas:
- SeparaÃ§Ã£o de responsabilidades
- ConfiguraÃ§Ã£o centralizada
- CÃ³digo modular e reutilizÃ¡vel
- Tratamento de erros robusto
- Interface CLI intuitiva

## LicenÃ§a

Este projeto foi desenvolvido como parte do desafio do MBA Engenharia de Software com IA - Full Cycle.
